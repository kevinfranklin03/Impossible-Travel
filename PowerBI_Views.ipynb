{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271906a0-43f8-4207-acdb-46a09a6c8adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ View created: powerbi_fraud_summary\n   Use for: KPI cards (total alerts, amount, cards flagged)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Power BI View #1: KPI Summary\n",
    "# ============================================\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW fraud_detection.raw.powerbi_fraud_summary AS\n",
    "SELECT \n",
    "    COUNT(*) as total_fraud_alerts,\n",
    "    ROUND(SUM(amount), 2) as total_fraud_amount,\n",
    "    COUNT(DISTINCT card_id) as unique_cards_flagged,\n",
    "    ROUND(AVG(speed_kmh), 0) as avg_impossible_speed,\n",
    "    ROUND(MAX(speed_kmh), 0) as max_speed_detected,\n",
    "    SUM(CASE WHEN severity = 'CRITICAL' THEN 1 ELSE 0 END) as critical_alerts,\n",
    "    SUM(CASE WHEN severity = 'HIGH' THEN 1 ELSE 0 END) as high_alerts,\n",
    "    MAX(alert_timestamp) as last_alert_time\n",
    "FROM fraud_detection.raw.gold_fraud_alerts\n",
    "WHERE alert_timestamp >= CURRENT_TIMESTAMP() - INTERVAL 24 HOUR\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ View created: powerbi_fraud_summary\")\n",
    "print(\"   Use for: KPI cards (total alerts, amount, cards flagged)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0c8852-cf8c-429f-9ad9-93bcde56aa45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ View created: powerbi_fraud_trends\n   Use for: Line chart (fraud over time)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Power BI View #2: Hourly Fraud Trends\n",
    "# ============================================\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW fraud_detection.raw.powerbi_fraud_trends AS\n",
    "SELECT \n",
    "    DATE_TRUNC('hour', alert_timestamp) as hour,\n",
    "    COUNT(*) as fraud_count,\n",
    "    ROUND(SUM(amount), 2) as total_amount,\n",
    "    ROUND(AVG(speed_kmh), 0) as avg_speed,\n",
    "    COUNT(DISTINCT card_id) as unique_cards,\n",
    "    severity\n",
    "FROM fraud_detection.raw.gold_fraud_alerts\n",
    "WHERE alert_timestamp >= CURRENT_TIMESTAMP() - INTERVAL 7 DAY\n",
    "GROUP BY DATE_TRUNC('hour', alert_timestamp), severity\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ View created: powerbi_fraud_trends\")\n",
    "print(\"   Use for: Line chart (fraud over time)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fde2cccc-0f82-42f9-a821-46dba44f7f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a02a759b-fbc7-44d6-88be-c89352d4911d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ View created: powerbi_top_fraud_cards\n   Use for: Table visual (top fraud cards)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Power BI View #4: Top Fraud Cards\n",
    "# ============================================\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW fraud_detection.raw.powerbi_top_fraud_cards AS\n",
    "SELECT \n",
    "    card_id,\n",
    "    COUNT(*) as fraud_count,\n",
    "    ROUND(SUM(amount), 2) as total_amount,\n",
    "    ROUND(AVG(speed_kmh), 0) as avg_speed,\n",
    "    MAX(severity) as max_severity,\n",
    "    MAX(alert_timestamp) as last_alert\n",
    "FROM fraud_detection.raw.gold_fraud_alerts\n",
    "WHERE alert_timestamp >= CURRENT_TIMESTAMP() - INTERVAL 7 DAY\n",
    "GROUP BY card_id\n",
    "ORDER BY fraud_count DESC\n",
    "LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ View created: powerbi_top_fraud_cards\")\n",
    "print(\"   Use for: Table visual (top fraud cards)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "657be3d6-b5fc-490c-9baa-1c07acf7d229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ View created: powerbi_system_performance\n   Use for: Line chart (latency over time)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Power BI View #5: System Performance\n",
    "# ============================================\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW fraud_detection.raw.powerbi_system_performance AS\n",
    "SELECT \n",
    "    batch_timestamp,\n",
    "    transactions_processed,\n",
    "    fraud_detected,\n",
    "    ROUND(processing_time_sec, 2) as processing_sec,\n",
    "    ROUND(avg_latency_ms, 0) as latency_ms,\n",
    "    ROUND((fraud_detected * 100.0 / NULLIF(transactions_processed, 0)), 2) as fraud_rate_pct\n",
    "FROM fraud_detection.raw.gold_system_performance\n",
    "WHERE batch_timestamp >= CURRENT_TIMESTAMP() - INTERVAL 24 HOUR\n",
    "ORDER BY batch_timestamp DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ View created: powerbi_system_performance\")\n",
    "print(\"   Use for: Line chart (latency over time)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a2e1ab-b7ff-4ef9-adc0-cd81d173c1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA TESTING ALL POWER BI VIEWS\n======================================================================\n✅ powerbi_fraud_summary: 1 rows\n✅ powerbi_fraud_trends: 2 rows\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4990283572869452>, line 17\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m views \u001B[38;5;241m=\u001B[39m [\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpowerbi_fraud_summary\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpowerbi_fraud_trends\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     13\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpowerbi_system_performance\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     14\u001B[0m ]\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m view \u001B[38;5;129;01min\u001B[39;00m views:\n",
       "\u001B[0;32m---> 17\u001B[0m     count \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfraud_detection.raw.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mview\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\n",
       "\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mview\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcount\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m rows\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1917\u001B[0m, in \u001B[0;36mSparkSession.table\u001B[0;34m(self, tableName)\u001B[0m\n",
       "\u001B[1;32m   1911\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tableName, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[1;32m   1912\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   1913\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_STR\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1914\u001B[0m         messageParameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtableName\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(tableName)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   1915\u001B[0m     )\n",
       "\u001B[0;32m-> 1917\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mtable(tableName), \u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:275\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    271\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    273\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    274\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 275\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `fraud_detection`.`raw`.`powerbi_fraud_geography` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01;\n",
       "'UnresolvedRelation [fraud_detection, raw, powerbi_fraud_geography], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `fraud_detection`.`raw`.`powerbi_fraud_geography` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01;\n'UnresolvedRelation [fraud_detection, raw, powerbi_fraud_geography], [], false\n"
       },
       "metadata": {
        "errorSummary": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `fraud_detection`.`raw`.`powerbi_fraud_geography` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "TABLE_OR_VIEW_NOT_FOUND",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "42P01",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-4990283572869452>, line 17\u001B[0m\n\u001B[1;32m      8\u001B[0m views \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpowerbi_fraud_summary\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpowerbi_fraud_trends\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpowerbi_system_performance\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     14\u001B[0m ]\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m view \u001B[38;5;129;01min\u001B[39;00m views:\n\u001B[0;32m---> 17\u001B[0m     count \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfraud_detection.raw.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mview\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mview\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcount\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m rows\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1917\u001B[0m, in \u001B[0;36mSparkSession.table\u001B[0;34m(self, tableName)\u001B[0m\n\u001B[1;32m   1911\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tableName, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m   1912\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   1913\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_STR\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1914\u001B[0m         messageParameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtableName\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(tableName)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   1915\u001B[0m     )\n\u001B[0;32m-> 1917\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mtable(tableName), \u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:275\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    271\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 275\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `fraud_detection`.`raw`.`powerbi_fraud_geography` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01;\n'UnresolvedRelation [fraud_detection, raw, powerbi_fraud_geography], [], false\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# Test All Power BI Views\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA TESTING ALL POWER BI VIEWS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "views = [\n",
    "    \"powerbi_fraud_summary\",\n",
    "    \"powerbi_fraud_trends\",\n",
    "    \"powerbi_fraud_geography\",\n",
    "    \"powerbi_top_fraud_cards\",\n",
    "    \"powerbi_system_performance\"\n",
    "]\n",
    "\n",
    "for view in views:\n",
    "    count = spark.table(f\"fraud_detection.raw.{view}\").count()\n",
    "    print(f\"✅ {view}: {count} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ ALL VIEWS READY FOR POWER BI!\")\n",
    "print(\"\\n\uD83D\uDCDD Power BI Connection Details:\")\n",
    "print(\"   Server: <your-workspace>.azuredatabricks.net\")\n",
    "print(\"   HTTP Path: /sql/1.0/warehouses/<warehouse-id>\")\n",
    "print(\"   Catalog: fraud_detection\")\n",
    "print(\"   Schema: raw\")\n",
    "print(\"\\n\uD83D\uDCA1 In Power BI, connect to these 5 views\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PowerBI_Views",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}