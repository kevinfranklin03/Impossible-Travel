{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1696e6b7-6a3d-41de-ab62-491a6df65524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SLA Monitoring loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SLA Monitoring Configuration\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "CATALOG = \"fraud_detection\"\n",
    "SCHEMA = \"raw\"\n",
    "\n",
    "SLA_METRICS_TABLE = f\"{CATALOG}.{SCHEMA}.gold_sla_metrics\"\n",
    "PERFORMANCE_TABLE = f\"{CATALOG}.{SCHEMA}.gold_system_performance\"\n",
    "\n",
    "print(\"✅ SLA Monitoring loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a108fa4f-4e56-43ca-8a41-5e620a335da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SLA metrics table created: fraud_detection.raw.gold_sla_metrics\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Create SLA Metrics Table\n",
    "# ============================================\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {SLA_METRICS_TABLE} (\n",
    "    check_timestamp TIMESTAMP,\n",
    "    metric_name STRING,\n",
    "    metric_value DOUBLE,\n",
    "    sla_target DOUBLE,\n",
    "    status STRING,\n",
    "    breach_severity STRING,\n",
    "    message STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✅ SLA metrics table created: {SLA_METRICS_TABLE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c30990-29c7-452a-8a09-97a2c5d26c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA SLA CHECK #1: LATENCY\n======================================================================\n⚠️  No data available yet\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SLA Check #1: End-to-End Latency < 5 seconds\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA SLA CHECK #1: LATENCY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get p95 latency from last hour\n",
    "latency_result = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        PERCENTILE(avg_latency_ms, 0.95) as p95_latency_ms,\n",
    "        AVG(avg_latency_ms) as avg_latency_ms,\n",
    "        MAX(avg_latency_ms) as max_latency_ms\n",
    "    FROM {PERFORMANCE_TABLE}\n",
    "    WHERE batch_timestamp >= CURRENT_TIMESTAMP() - INTERVAL 1 HOUR\n",
    "\"\"\").first()\n",
    "\n",
    "if latency_result and latency_result.p95_latency_ms:\n",
    "    p95_latency = latency_result.p95_latency_ms\n",
    "    sla_target = 5000.0  # 5 seconds\n",
    "    \n",
    "    status = \"✅ PASS\" if p95_latency <= sla_target else \"❌ BREACH\"\n",
    "    severity = \"CRITICAL\" if p95_latency > sla_target * 1.5 else \"WARNING\" if p95_latency > sla_target else \"OK\"\n",
    "    \n",
    "    print(f\"Target: < {sla_target:.0f}ms (p95)\")\n",
    "    print(f\"Current p95: {p95_latency:.0f}ms\")\n",
    "    print(f\"Current avg: {latency_result.avg_latency_ms:.0f}ms\")\n",
    "    print(f\"Current max: {latency_result.max_latency_ms:.0f}ms\")\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    # Save result\n",
    "    spark.createDataFrame([{\n",
    "        \"check_timestamp\": datetime.now(),\n",
    "        \"metric_name\": \"p95_latency_ms\",\n",
    "        \"metric_value\": float(p95_latency),\n",
    "        \"sla_target\": sla_target,\n",
    "        \"status\": \"PASS\" if p95_latency <= sla_target else \"BREACH\",\n",
    "        \"breach_severity\": severity,\n",
    "        \"message\": f\"p95 latency: {p95_latency:.0f}ms (target: {sla_target:.0f}ms)\"\n",
    "    }]).write.format(\"delta\").mode(\"append\").saveAsTable(SLA_METRICS_TABLE)\n",
    "else:\n",
    "    print(\"⚠️  No data available yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62db779f-35f6-4859-a1ca-b5fa89bb0e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA SLA CHECK #2: DATA FRESHNESS\n======================================================================\n⚠️  No data available yet\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SLA Check #2: Data Freshness < 1 minute\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA SLA CHECK #2: DATA FRESHNESS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "latest_batch = spark.sql(f\"\"\"\n",
    "    SELECT MAX(batch_timestamp) as latest\n",
    "    FROM {PERFORMANCE_TABLE}\n",
    "\"\"\").first()\n",
    "\n",
    "if latest_batch and latest_batch.latest:\n",
    "    freshness_sec = (datetime.now() - latest_batch.latest).total_seconds()\n",
    "    sla_target = 60.0  # 1 minute\n",
    "    \n",
    "    status = \"✅ PASS\" if freshness_sec <= sla_target else \"❌ BREACH\"\n",
    "    severity = \"CRITICAL\" if freshness_sec > 300 else \"WARNING\" if freshness_sec > sla_target else \"OK\"\n",
    "    \n",
    "    print(f\"Target: < {sla_target:.0f} seconds\")\n",
    "    print(f\"Current: {freshness_sec:.0f} seconds\")\n",
    "    print(f\"Last batch: {latest_batch.latest}\")\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    # Save result\n",
    "    spark.createDataFrame([{\n",
    "        \"check_timestamp\": datetime.now(),\n",
    "        \"metric_name\": \"data_freshness_sec\",\n",
    "        \"metric_value\": freshness_sec,\n",
    "        \"sla_target\": sla_target,\n",
    "        \"status\": \"PASS\" if freshness_sec <= sla_target else \"BREACH\",\n",
    "        \"breach_severity\": severity,\n",
    "        \"message\": f\"Data freshness: {freshness_sec:.0f}s (target: {sla_target:.0f}s)\"\n",
    "    }]).write.format(\"delta\").mode(\"append\").saveAsTable(SLA_METRICS_TABLE)\n",
    "else:\n",
    "    print(\"⚠️  No data available yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f7239ff-f073-4eb0-b16c-0e104d3b5575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA SLA CHECK #3: THROUGHPUT\n======================================================================\n⚠️  No data available yet\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SLA Check #3: Throughput > 100 txns/minute\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA SLA CHECK #3: THROUGHPUT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "throughput_result = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        SUM(transactions_processed) / \n",
    "        (COUNT(*) * 10.0 / 60.0) as txns_per_minute\n",
    "    FROM {PERFORMANCE_TABLE}\n",
    "    WHERE batch_timestamp >= CURRENT_TIMESTAMP() - INTERVAL 1 HOUR\n",
    "\"\"\").first()\n",
    "\n",
    "if throughput_result and throughput_result.txns_per_minute:\n",
    "    throughput = throughput_result.txns_per_minute\n",
    "    sla_target = 100.0  # 100 transactions per minute\n",
    "    \n",
    "    status = \"✅ PASS\" if throughput >= sla_target else \"❌ BREACH\"\n",
    "    \n",
    "    print(f\"Target: > {sla_target:.0f} txns/min\")\n",
    "    print(f\"Current: {throughput:.0f} txns/min\")\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    # Save result\n",
    "    spark.createDataFrame([{\n",
    "        \"check_timestamp\": datetime.now(),\n",
    "        \"metric_name\": \"throughput_txns_per_min\",\n",
    "        \"metric_value\": throughput,\n",
    "        \"sla_target\": sla_target,\n",
    "        \"status\": \"PASS\" if throughput >= sla_target else \"BREACH\",\n",
    "        \"breach_severity\": \"WARNING\" if throughput < sla_target else \"OK\",\n",
    "        \"message\": f\"Throughput: {throughput:.0f} txns/min (target: {sla_target:.0f})\"\n",
    "    }]).write.format(\"delta\").mode(\"append\").saveAsTable(SLA_METRICS_TABLE)\n",
    "else:\n",
    "    print(\"⚠️  No data available yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b4f075d-8179-486b-81e5-93056f22333c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA SLA SUMMARY (Last 24 Hours)\n======================================================================\n+-----------+------------+------+--------+---------+------+\n|metric_name|total_checks|passed|breached|avg_value|target|\n+-----------+------------+------+--------+---------+------+\n+-----------+------------+------+--------+---------+------+\n\n\n✅ ALL SLAs MET - System performing within targets!\n\n\uD83D\uDCA1 Schedule this notebook to run every 15 minutes\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SLA Summary - Last 24 Hours\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA SLA SUMMARY (Last 24 Hours)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        metric_name,\n",
    "        COUNT(*) as total_checks,\n",
    "        SUM(CASE WHEN status = 'PASS' THEN 1 ELSE 0 END) as passed,\n",
    "        SUM(CASE WHEN status = 'BREACH' THEN 1 ELSE 0 END) as breached,\n",
    "        ROUND(AVG(metric_value), 2) as avg_value,\n",
    "        MAX(sla_target) as target\n",
    "    FROM {SLA_METRICS_TABLE}\n",
    "    WHERE check_timestamp >= CURRENT_TIMESTAMP() - INTERVAL 24 HOUR\n",
    "    GROUP BY metric_name\n",
    "\"\"\")\n",
    "\n",
    "summary.show(truncate=False)\n",
    "\n",
    "# Check for breaches\n",
    "breach_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as breaches\n",
    "    FROM {SLA_METRICS_TABLE}\n",
    "    WHERE check_timestamp >= CURRENT_TIMESTAMP() - INTERVAL 24 HOUR\n",
    "    AND status = 'BREACH'\n",
    "\"\"\").first().breaches\n",
    "\n",
    "if breach_count > 0:\n",
    "    print(f\"\\n⚠️  {breach_count} SLA BREACHES in last 24 hours!\")\n",
    "    \n",
    "    # Show recent breaches\n",
    "    print(\"\\n\uD83D\uDEA8 Recent breaches:\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT check_timestamp, metric_name, metric_value, sla_target, message\n",
    "        FROM {SLA_METRICS_TABLE}\n",
    "        WHERE status = 'BREACH'\n",
    "        AND check_timestamp >= CURRENT_TIMESTAMP() - INTERVAL 24 HOUR\n",
    "        ORDER BY check_timestamp DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").show(truncate=False)\n",
    "else:\n",
    "    print(\"\\n✅ ALL SLAs MET - System performing within targets!\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 Schedule this notebook to run every 15 minutes\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SLA_Monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}